---
title: 'HPC - Parallel Computing Basics'
description: 'GPU Architectureë¥¼ ì‚´í´ë³´ê¸° ì „ì— "Parallel Architecture"ê°€ ë¬´ì—‡ì¸ì§€'
date: 2024-05-10
technologies: ['Astro', 'TypeScript', 'Obsidian']
repoLink: 'https://github.com/example/life-archive'
---

GPU Architectureë¥¼ ì‚´í´ë³´ê¸° ì „ì— "**Parallel Architecture**"ê°€ ë¬´ì—‡ì¸ì§€, ì´ëŸ¬í•œ êµ¬ì¡°ë¥¼ "**Parallel**"ë¡œ ì‹¤í–‰í•˜ê¸° ìœ„í•´ì„œëŠ” í”„ë¡œê·¸ë¨ì„ ì‘ì„±í•˜ëŠ” ê²ƒì´ ë¬´ì—‡ì¸ì§€ë¥¼ ì‚´í´ë³´ë ¤ê³  í•œë‹¤.
-> It will help us to realize why we need gpu architecture
-> ìš°ë¦¬ê°€ ì§„ì§œ Parallelì„ í•˜ê¸° ìœ„í•´ì„œëŠ” ì–´ë–»ê²Œ softwareë¥¼ ì‘ì„±í•´ì•¼ í•˜ëŠ”ì§€ì— ëŒ€í•´ì„œ ë°°ìš°ë ¤ê³  í•œë‹¤.

# â€‹â€‹â€‹Types of Parallellism

ì–´ë–¤ ê±¸ ë³‘ë ¬ì²˜ë¦¬ í•œë‹¤ëŠ” ê±´ -> ì—¬ëŸ¬ê°œì˜ ë¦¬ì†ŒìŠ¤ë¥¼ ì‚¬ìš©í•´ì„œ
TVë¥¼ ë³´ëŠ” ê²ƒê³¼ ìˆ™ì œë¥¼ í•˜ëŠ” ê°™ì´ í•˜ëŠ” ê±´ -> context switch  
í¬ê²Œ ë³´ë©´ Fine-grained Instruction Parallelismê³¼ Higher abstarction level ì´ ì¡´ì¬í•œë‹¤

- ILP(Low Level) - Single Cycleì— more than one instruction
  ëŒ€ë¶€ë¶„ í•˜ë“œì›¨ì–´ì— ì˜í•´ì„œ ë‹¤ë£¨ì–´ì§„ë‹¤. -> í•˜ë“œì›¨ì–´ê°€ can be executed in parallelì„ ì°¾ì•„ì„œ ë³‘ë ¬ë¡œ ì‹¤í–‰ (ë‹¨ìœ„ëŠ” **Instruction Window**ë¥¼ ê¸°ë°˜ìœ¼ë¡œ)
  ex) **Superscalar** - Can issue multiple instruction at a time
  -> í•œêº¼ë²ˆì— Issueê°€ ë  ìˆ˜ ìˆëŠ” Instructionì„ ì°¾ì•„ì„œ ìœ„ì¹˜ì‹œí‚¨ë‹¤. (ì—¬ê¸°ì—ëŠ” Dependencyê°€ ì¡´ì¬í•˜ì§€ X)
  -> ì´ ê³¼ì •ì—ì„œ ìš°ë¦¬ëŠ” ì–´ë–»ê²Œ Parallelismì´ ì§„í–‰ë˜ëŠ”ì§€ ì•Œ í•„ìš”ê°€ ì—†ë‹¤.
  -> Compilerê°€ Instructionì„ ì¬êµ¬ì„±í•  ìˆ˜ë„ ìˆì§€ë§Œ ëŒ€ê°œ ì´ê±´ í•˜ë“œì›¨ì–´ê°€ ì²˜ë¦¬ë¥¼ í•œë‹¤.

  **"ê°œë°œì ê´€ì " : Codeì—ì„œ ì•„ë¬´ê±±ì •ì„ í•˜ì§€ ì•Šì•„ë„ ëœë‹¤.**

  ***

- TLP(High Level)
  ThreadëŠ” í•˜ë“œì›¨ì–´ì— ì¡´ì¬í•˜ì§€ ì•ŠëŠ”ë‹¤. == í•˜ë“œì›¨ì–´ëŠ” Only Instructionë§Œ ì´í•´í•œë‹¤.
  Thread ê°œë… ìì²´ë¥¼ ì‚¬ìš©í•œë‹¤ëŠ” ê±´ Softwareê¸°ë°˜ì˜ ì‘ì—…ì´ë¼ëŠ” ê²ƒì´ë‹¤.
  -> Parallelismì€ explictly í•˜ê²Œ í”„ë¡œê·¸ë¨ì„ ì—¬ëŸ¬ ì¡°ê°ìœ¼ë¡œ ë‚˜ëˆ ì•¼í•œë‹¤.
  **"ê°œë°œì ê´€ì "** : Explictí•˜ê²Œ ìš°ë¦¬ê°€ ë¬´ì—‡ì„ í•´ì•¼í•˜ëŠ”ì§€ í‘œí˜„í•´ì¤˜ì•¼ í•œë‹¤.
  ***
- DLP(High Level)
  GPUì—ì„œ DLPí˜•íƒœì˜ Parallelismì„ ì œê³µí•´ì„œ ê´€ì‹¬ì„ ê°€ì ¸ì•¼ í•œë‹¤.
  DLPëŠ” ë³¸ì§ˆì ìœ¼ë¡œ TLP í˜•íƒœì˜ ì¼ë¶€ë¶„ì´ë‹¤.
  -> Constrained TLPêµ¬ì¡°ì´ê³  ëª¨ë“  ìŠ¤ë ˆë“œê°€ ë‹¤ë¥¸ ë°ì´í„°ë¥¼ ê°€ì§€ê³  ê°™ì€ í–‰ë™ì„ í•˜ëŠ” ê²ƒì´ë‹¤. **"ê°œë°œì ê´€ì "** : Vector Instruction(load all the values)
  ***

# Flynn's Taxonomy

SISD - Oridinaty CPU (ILP)
MISD - X (not real world)
SIMD - DLP (Single Instruction poll, Single Thread)
MIMD - TLP(ê° CPUëŠ” ë‹¤ë¥¸ ë°ì´í„°ë¥¼ ê°€ì§€ê³  ë‹¤ë¥¸ ì‘ì—…ì„ í•œë‹¤.) (e.g., multiple CPUs, Cluster level parallelism)

# How to Extract Parallelism in Hardware

> í•˜ë“œì›¨ì–´ì—ì„œ ë³‘ë ¬ì„±ì„ ì–´ë–»ê²Œ ì´ëŒì–´ë‚´ê³  í™œìš©í•  ìˆ˜ ìˆì„ê¹Œ?

---

## ILP

Superscalar - **multiple instructions** issued at **a single cycle** from **a single thread**
ì•„ì§ multiple threadë¼ëŠ” ê°œë…ì´ ì•„ì§ X
ILPëŠ” ì˜¤ì§ Instruction Window(ì´ê±¸ ë„˜ì–´ê°ˆ ìˆ˜ ì—†ìŒ)ì—ë§Œ ì˜ì¡´í•œë‹¤.
-> ë§Œì•½ì´ Window ì‚¬ì´ì¦ˆ ë§Œí¼ ë³‘ë ¬ì²˜ë¦¬ë¥¼ ëª»í•˜ê³  ìˆë‹¤ëŠ” ê±´ ì–´ë–¤ functional unitsì´ ë†€ê³  ìˆë‹¤ëŠ” ê²ƒì…ë‹ˆë‹¤.

---

## TLP

- Chip Multi Processors(CMP) - **Multi-Core**(Have own seperate issue, PC, register)
  **`Each Thread - Each Core`**
- **FIne-grained Multithreading - Single-Core**( **`Multiple Threads - Each Core`**)
  Issue instruction from different threads at different cycle
  -> ë§Œì•½ FIne-grainedê°€ ì—†ìœ¼ë©´ ì˜¤ì§ 1ê°œì˜ Threadì˜ instructionë°–ì— issueí•˜ì§€ ëª»í•œë‹¤.
  -> ì´ ë°©ì‹ì´ ì—†ìœ¼ë©´ ë‹¨ì¼ ìŠ¤ë ˆë“œì˜ ëª…ë ¹ì–´ë§Œ issueí•  ìˆ˜ ìˆëŠ”ë°, ê·¸ ìŠ¤ë ˆë“œê°€ **ë©”ëª¨ë¦¬ ì ‘ê·¼ì´ë‚˜ I/O ê°™ì€ ì¥ê¸° ì§€ì—°(latency)ì„ ìœ ë°œí•˜ëŠ” ì—°ì‚°ì„ ìˆ˜í–‰**í•˜ë©´, í•´ë‹¹ ìŠ¤ë ˆë“œê°€ stallë˜ëŠ” ë™ì•ˆ **CPUëŠ” ì•„ë¬´ ì¼ë„ í•˜ì§€ ëª»í•˜ê³  idle ìƒíƒœ**ê°€ ë©ë‹ˆë‹¤.
  > [!info] OS Thread Context Switchë‘ì€ ë­ê°€ ë‹¤ë¥¸ê±´ê°€?
  > ![[Pasted image 20250508151436.png]]
- Simultaneous MultiThreading(SMT) - **Single-Core**
  more aggresive version of a fine-grain multithreading
  Cycle ì¡°ê°ìœ¼ë¡œ ì—¬ëŸ¬ Threadë¥¼ ë³‘ë ¬í™”í•˜ëŠ” ê²ƒì´ ì•„ë‹ˆë¼ Same Cycleì— Mix&Match
  multi-coreë¼ëŠ” ê°œë…ì´ ë‚˜ì˜¤ê¸° ì „ë¶€í„° ì ìš©ì´ ë˜ì—ˆê³  ëŒ€ë¶€ë¶„ì— CPUëŠ” SMTë¥¼ ì§€ì›í•œë‹¤. **`Multiple Threads - Each Core`**

---

## DLP

- Vector Processor(SIMD)
  Register, Memory range - Load amount of data from memory, execute everything at the same register
  single instuction - multiple pieces of data
  **`Each Thread - Each Core - Multiple Data`**

---

# Microprocessor Architecture Trends

#### **ğŸ”¹ Single Threaded**

- **CISC Machines**
  Single Instructionë§ì€ ê²ƒë“¤ì„ í•˜ê³  ê·¸ë˜ì„œ Complex Instructionì´ë¼ê³  í•˜ëŠ” ê±°ë‹¤.
- **RISC Machines(microcode)**
  ì—¬ê¸°ì„œëŠ” instructionì˜ ì–‘ì„ ì¤„ì˜€ê³  **Simple Instruction**
  One Thing at a time
- **RISC Machines(pipelined)**
  Same Instructionì´ì§€ë§Œ ìš°ë¦¬ëŠ” ì´ê±¸ ì—¬ëŸ¬ ì¡°ê°ìœ¼ë¡œ ìª¼ê°œì„œ ì—¬ëŸ¬ ë‹¨ê³„ë¡œ í‘œí˜„
  Fetch Decode Excute Load
  Up to this point it's all single threaded

#### **ğŸ”¹ ILP**

- **RISC Machines(pipelined)**
- SuperScalar Processor

#### **ğŸ”¹ Programs that have to be parallel**

- **Using a single core by multiple threads**
  - **Multithreaded Processor(Fine-Grained MultiThreading)**
    hardware has set of hardware resources to differentiate between different threads (PC, Register, stack pointer)
    í•œ ì‚¬ì´í´ì—ì„œëŠ” ì˜¤ì§ í•œê°œì˜ ìŠ¤ë ˆë“œë§Œ ë™ì‘í•˜ê¸´ í•œë‹¤.(ì—¬ëŸ¬ê°œì˜ ìŠ¤ë ˆë“œëŠ” context switch)
    --> ì´ê±´ GPUê°€ DLPì„ í•˜ëŠ” ê³¼ì •ì—ì„œ ì´ ë°©ì‹ì„ ì‚¬ìš©í•˜ê¸° ë•Œë¬¸ì— ë‚˜ì¤‘ì— ì°¸ê³ í•  ê²ƒ
  - **SMT**
    Each Cycle, Any Context max execute
- Using a single core by Single or multiple threads
  - VLIW(Very Long Instruction word)
    Move the responsiblity to Compiler
    Compilerê°€ ì½”ë“œë¥¼ ë¶„ì„í•˜ê³  ì˜ì¡´ì„±ì„ í™•ì¸í•´ì„œ ì—¬ëŸ¬ê°œì˜ ë³‘ë ¬ ë¬µìŒì„ ìƒì„±
    í•˜ë“œì›¨ì–´ê°€ ì²´í•˜ì§€ ì•ŠëŠ”ë‹¤.
    ILPì˜ ë‹¤ë¥¸ í˜•íƒœì´. AMD ARMì˜ Early
- Using a multiple cores by multiple threads
  CMPs
  -> CMPë‚˜ SMT ë‘˜ë‹¤ Each Coreì—ì„œ ì—¬ëŸ¬ê°œì˜ ìŠ¤ë ˆë“œê°€ (Chip level tlp)

#### **ğŸ”¹ Vector computers**

#### **ğŸ”¹ GPU**

# Parallelism vs. Concurrency

ë³‘ë ¬ì„±ì€ the pair of resources
sorting problem - divide & conquer (parallelism)

Concurrency ì—¬ëŸ¬ê°œì˜ jobì„ at the same time
ìŠ¤ë ˆë“œë¼ë¦¬ì˜ ì»¨í…ìŠ¤íŠ¸ ìŠ¤ìœ„ì¹˜
ìœ ì €ì…ì¥ì—ì„œëŠ” ê·¸ë ‡ê²Œ ë³´ì¸ë‹¤ ê°™ì´ ì‘ë™í•˜ëŠ” ê²ƒì²˜ëŸ¼
ê° ìŠ¤ë ˆë“œê°€ ê° CPUë¥¼ ê°€ì§„ ê²ƒì²˜ëŸ¼

# Where the hardware actually provide the parallelism

í”„ë¡œê·¸ë˜ë¨¸ëŠ” ë™ì‹œì„±ì„ ì¸ì‹í•´ì„œ ì•Œë ¤ì£¼ëŠ” ê²ƒì´ë‹¤. (ì•Œê³ ë¦¬ì¦˜ ë ˆë²¨ì—ì„œ)
ì—¬ëŸ¬ ì—°ì‚°, ì»´í“¨í„°ëŠ” ë³‘ë ¬í™” í•˜ëŠ”ë°
ì—¬ê¸°ì„œ ì•Œê³ ë¦¬ì¦˜ ë³€í™”ê°€ í•„ìš”í• ê±°ë‹¤. -> ë™ì‹œì„±ì„ ìµœëŒ€í™”í•  ìˆ˜ ìˆê²Œ ì†Œí”„íŠ¸ì›¨ì–´ ë ˆë²¨ì—ì„œ

## Problems

1. ì˜ì¡´ì„±()
2. 80ëª…ì´ ì´ ì¼ì„ ë‚˜ëˆ„ë©´ ë¹„íš¨ìœ¨ì  ì´ ê³¼ì •ì—ì„œ ë§¤ë‹ˆì§• í•  ë•Œ
   ê°€ë” ì˜¤ë˜ ê±¸ë¦´ ìˆ˜ë„ ìˆë‹¤.
3. ì¼ì„ ì˜ ë‚˜ëˆ„ëŠ” ê²Œ ì¢‹ë‹¤.

# Parallel Algorithm Design

ë³‘ë ¬ í”„ë¡œê·¸ë¨ì„ ì‚¬ìš©í•  ë•Œ ì–´ë–¤ ê²ƒë“¤ì„ ìƒê°í•´ì•¼ í•˜ëŠ”ì§€ì— ëŒ€í•´ì„œ ë‹¤ë£¬ë‹¤.

1. ë³‘ë ¬ë¡œ ì²˜ë¦¬í•  ìˆ˜ ìˆëŠ” ì—°ì‚°ì´ ë­ê°€ ìˆëŠ”ì§€ ì°¾ëŠ”ë‹¤.
2. ì´ ì¼ì„ ì–´ë–»ê²Œ íŒŒí‹°ì…˜ í• ì§€(Data)
3. ê·¸ë¦¬ Manage

ì—¬ê¸°ì„œ ì´ê²Œ ì–¼ë§ˆë‚˜ ì»¤ì•¼í•˜ëŠ”ì§€, ê·¸ë¦¬ê³  ì–¸ì œ ì´ í¬ê¸°ê°€ ì ë‹¹í•œì§€ë¥¼ ê²°ì •í•  ê²ƒì¸ì§€
**íƒ€ì´ë°, granality Issue**

ê·¸ë¦¬ê³  ë‚˜ì„œ ì‹±í¬ë¡œë‚˜ì´ì¦ˆ

ì–´ë–»ê²Œ ë§µí•‘í•˜ëŠ”ì§€ì— ë”°ë¼ì„œ ì°¨ì´ê°€ ë§ì´ ë‚œë‹¤.
ëª¨ë“  ê³¼ì •ì´ ë³‘ë ¬ì˜ ì§ˆì„ ë‚˜ëˆ„ëŠ” ë° ì‚¬ìš©ëœë‹¤.

# Task Decomposition

ì–´ë–¤ í•´ê²°í•´ì•¼ í•˜ëŠ” ë¬¸ì œë¥¼ "chunks"ì˜ ì‘ì—… ë‹¨ìœ„ë¡œ ë‚˜ëˆ„ëŠ” ì‘ì—…ì…ë‹ˆë‹¤.
ê¸°ëŠ¥ ë˜ëŠ” ì—°ì‚° ë‹¨ìœ„ë¥¼ ê¸°ì¤€ìœ¼ë¡œ ë‚˜ëˆ„ëŠ” ê±¸ ì–˜ê¸°í•˜ê³  ì´ê±°ë¥¼ ë‚˜ì¤‘ì— ë°ì´í„° Decompositionìœ¼ë¡œ ë‚˜ëˆ„ëŠ” ì‘ì—…ì´ë‹¤.

> [!info] **Main Idea**
> Create at least enough tasks to keep execution units on a machine busy

Threadì˜ ê°œìˆ˜ê°€ ë§ê³  Thread í•˜ë‚˜ê°€ í•´ì•¼í•˜ëŠ” ì¼ì€ ë„ˆë¬´ ì‘ë‹¤.
ì“°ë ˆë“œë¥¼ ë§Œë“œëŠ” ê³¼ì •, ìˆ˜í–‰í•˜ëŠ” ê³¼ì •, ëë‚´ëŠ” ê³¼ì • ëª¨ë‘ Management Overheadê°€ ë“ ë‹¤.
(Threadë¼ì´ë¸ŒëŸ¬ë¦¬ë¥¼ ì¨ì„œ ê´€ë¦¬í•  ìˆ˜ë„ ìˆê³ , OSì—ì„œ ê´€ë¦¬í•  ìˆ˜ë„ ìˆë‹¤)
-> ì“°ë ˆë“œê°€ í•œ ë²ˆ ìƒì„±ë˜ë©´ ì¼ì • ìˆ˜ì¤€ì˜ ì–‘ì„ í•˜ê²Œ í•  ìˆ˜ë„ ìˆë‹¤. (4 rows per thread)
-> **Problem of task granularity**

ë§Œì•½ì— 2 Row, Half of Col -> **Syncronize**

# Options of Decomposition

## How to Decomposition

- Domain Decomposition(DLP) - Partitioning
  ë°ì´í„°ì…‹ì„ ì‘ì€ ì¡°ê°ë“¤ë¡œ ë‚˜ëˆ„ëŠ” ê²½ìš°
- Funtional Decomposition
  Workë¥¼ Different Types of workë¡œ ë‚˜ëˆ„ëŠ” ê²½ìš°
  E.g., pipeline parallel algorithms

## When to Decomposition

- Static Decomposition
  Taskì™€ ë°ì´í„°ë¥¼ ì•Œê³  ìˆìœ¼ë©´ ë„ì›€ì´ ëœë‹¤.

- Dynamic Decomposition
  Sparse Matrixë¥¼ ìƒê°í•´ë³´ë©´ ì–´ë–¤ ê±´ ì—°ì‚°í•  ê²Œ ë§ê³  ì–´ë–¤ ê±´ ì ì€ ê²½ìš°ê°€ ìƒê¸¸ ìˆ˜ ìˆë‹¤.
  -> Load Imbalance í•´ê²° ê°€ëŠ¥
  í•˜ì§€ë§Œ ê·¸ ê³¼ì •ì—ì„œ íŠ¸ë™í‚¹í•´ì•¼í•˜ëŠ” ê²Œ ë§ê¸° ë–„ë¬¸ì— overhead ì¡´ì¬ ê°€ëŠ¥

## How large/small are tasks?

- Coarse-Grained Tasks
  Computation > Communication
  Low overhead in communication
  ì™œ? -> ì²­í¬ë¥¼ í¬ê²Œ ë‚˜ëˆ„ë©´ ì „ì²´ì ìœ¼ë¡œ ëë‚˜ëŠ” ì‹œê°„ì´ ìŠ¤ë ˆë“œë³„ë¡œ **imbalance**
- Fine-Grained Tasks
  Computation < Communication
  Load Balance
  **Parallel execution overheads = communication/synchronization + idling + excess work**

## Summary

ìš°ë¦¬ê°€ Decompositionì—ì„œ ì„ íƒí•  ìˆ˜ ìˆëŠ” ì˜µì…˜ì€ ì´ 3ê°€ì§€ê°€ ì¡´ì¬í•œë‹¤.

1. ì–´ë–»ê²Œ ë‚˜ëˆŒ ê²ƒì¸ì§€
   - Task Decomposition
   - Data Decomposition
2. ì–¸ì œ ë‚˜ëˆŒ ê²ƒì¸ì§€
   - Static Decomposition
   - Dynamic Decomposition -> overhead, but load balance
3. ì–¼ë§ˆë‚˜ í¬ê²Œ ë‚˜ëˆŒ ê²ƒì¸ì§€
   - Coarse
   - Fine
     Fineì¼ìˆ˜ë¡ Load InbalanceëŠ” ê°€ëŠ¥í•˜ì§€ë§Œ Communication/Sync overheadê°€ ì¡´ì¬
     ![[Pasted image 20250508171301.png]]

# Mapping and Scheduling

Mappingì€ í•˜ë“œì›¨ì–´(Processing Unit)ì— Works(tasks)ë¥¼ í• ë‹¹í•˜ê³  - **Mapping**
Scedulingì€ ì–¸ì œ ê·¸ workê°€ ì‹¤í–‰ë˜ì–´ì•¼ í•˜ëŠ”ì§€ë¥¼ ëª…ì‹œí•˜ëŠ” ê²ƒ - Scheduling

> [!info] Total Overhead in Parallel Execution
> $Parallel\;Execution\;OverHead = Communication/Sync + Idling + Excess Work$
> $$T_0(Total Overhead) = p * T_p - T_s$$

## When to Mapping and Scheduling

- Static Mapping and Scheduling
  Taskì˜ ì‚¬ì´ì¦ˆ ë¶ˆë³€í•˜ê³  Taskê°€ ì •ì ìœ¼ë¡œ ìƒì„±ë˜ë©´ ì‚¬ìš© ê°€ëŠ¥
  OverheadëŠ” ì¤„ì¼ ìˆ˜ ìˆë‹¤.
  ![[Pasted image 20250508172128.png]]
- Dynamic mapping and scheduling
  Require tasks queue, task/data migration
  Higher Overhead, Reduce Load Imbalance
  ![[Pasted image 20250508172208.png]]

## Ways to minimize overheads

1. Map independent tasks to different processing units - **Maximize parallelism**
   -> ë§Œì•½ dependentí•œ taskë¥¼ ë„£ìœ¼ë©´ synchronizeí•´ì•¼í•˜ëŠ” ì˜¤ë²„í—¤ë“œ ì¡´ì¬(lock ... )
2. Assign tasks on critical path to processing units as soon as they become available - **Minimize load imbalance**
   -> **â€œê°€ì¥ ì¤‘ìš”í•œ ê²½ë¡œ(critical path)â€œì˜ ì‘ì—…ë“¤ì€ ê°€ëŠ¥í•œ ë¹¨ë¦¬ ì‹¤í–‰ë˜ë„ë¡ ìœ ë‹›ì´ ë¹„ìë§ˆì ë°°ì •í•˜ë¼**
3. Minimize communication by mapping process unit with dense interactions to the same processing units - **Minimize overheads**

ì´ ë°©ì‹ìœ¼ë¡œ ì‚¬ìš©í•˜ë ¤ê³  í•˜ë©´ ì„œë¡œ ì¶©ëŒì´ ë‚  ìˆ˜ë„ ìˆë‹¤.
ì¶©ë¶„í•œ ë…ë¦½ì ì¸ ì‘ì—…ì„ ì°¾ì„ ìˆ˜ ì—†ì„ ìˆ˜ë„ ìˆê³  Critical Pathë¥¼ ì¤„ì´ë ¤ê³  í•˜ë©´ ë‹¤ë¥¸ Processing Unitì— ë„£ì–´ì•¼ í•˜ê¸° ë•Œë¬¸ì— ì˜ì¡´ì„±ì´ ìƒê¸¸ ìˆ˜ë°–ì— ì—†ë‹¤.

# Communication and Synchronization

ë³‘ë ¬ Task ì‚¬ì´ì—ì„œ ë°ì´í„°ë¥¼ ê³µìœ í•˜ëŠ” ê²ƒ

### Communcation OverHead

- Prepare, Receive Shared Data
- Synchronization result in tasks waiting
- Contention from competing communicatio

### Data Sharing Method

- Message Passing Model(**Explicit**)
- Shared Memory Model(**Implicit**)
  Transparent to the programmer

### Types of Communication

- Collective(Multiple Entities)
  - Broadcast (one-to-all, all-to-all)
  - Multicast(To subset of threads)
  - Reduction (all-to-one)
  - Scatter and gather(not computing)
- Point-to-point(Give point to only one thread)
  - Lock
  - Producer-Consumer

### **Barrier Synchronization Primitives**

Dependencyë¥¼ í‘œí˜„í•˜ëŠ” ë³´ìˆ˜ì (conservative)ì¸ ë°©ë²•
**Computation ë‹¨ê³„ë¥¼ Phaseë¡œ ìª¼ê°œëŠ” ë°©ë²•**

### **Coarse-grained Example**

Barrier, Centralized Barrier - ë³€ìˆ˜ë¥¼ ë‘ê³  ì–¼ë§ˆë‚˜ ë§ì€ ìŠ¤ë ˆë“œê°€ barrierì— ë„ë‹¬í–ˆëŠ”ì§€ ì„¼ë‹¤.
-> sense, counterë¼ëŠ” ë³€ìˆ˜ë¥¼ ê°€ì§„ë‹¤.
-> ë§ˆì§€ë§‰ ìŠ¤ë ˆë“œê°€ senseë¥¼ ë’¤ì§‘ê³ , **globalí•˜ê²Œ broadcast**í•´ì¤ë‹ˆë‹¤.
ë‹¤ì–‘í•œ phaseë“¤ì˜ ë‹¨ê³„ë¡œ ë‚˜ë‰œë‹¤.

ì•„ë˜ ì˜ˆì‹œì—ì„œëŠ” ê³µìœ ë¥¼ í•˜ëŠ” 1ê°œì˜ Read-Only ë³€ìˆ˜ê°€ ì¡´ì¬í•˜ê³  ì—´ë‹¨ìœ„ ìŠ¤ë ˆë“œ ì‘ì—…ì´ ì¢…ë£Œê°€ ë˜ê³  ì´ì œ Rowë‹¨ìœ„ì˜ ì‘ì—…ì´ ì‹œì‘ë˜ëŠ”ë° ì •ë³´ë“¤ì´ P1,P2,P3ì—ì„œ ì „ë‹¬ ë˜ì–´ì•¼ í•œë‹¤.  
-> ê·¼ë° ì—¬ê¸°ì„œ Barrierì—ì„œëŠ” ëª¨ë“  í”„ë¡œì„¸ì„œê°€ ë©ˆì¶°ì•¼ í•˜ë‹ˆ ì˜ì¡´ì„±ì„ ê°•í•˜ê²Œ ì œí•œí•  ìˆ˜ ìˆë‹¤.

![[Pasted image 20250507161435.png]]

### **Fine-Grained Example**

![[Pasted image 20250507161924.png]]

### ì™œ(ì–¸ì œ) distributed Syncê°€ Centralized Syncë³´ë‹¤ ëŒ€ì²´ë¡œ ì¢‹ì„ê¹Œ?

Coreìˆ˜ê°€ ì ë‹¹íˆ ë§ìœ¼ë©´ Distributed Syncê°€ ë” ë‚«ë‹¤

# Performance Model

## Basic Measures of Parallelism

$$Total Overhead; \quad T_0\;=pT_p\;-\;T_s$$
$$SpeedUp; \quad S_p\;=T_s\;/\;T_p$$
$$Efficiency; \quad E\;=S_p\;/\;p$$
ë§Œì•½ì— $T_s$ê°€ 100ì´ˆì˜€ê³  ì½”ì–´ê°€ 5ê°œì¼ ë•Œ $T_p$ê°€ 25ì´ˆê°€ ë‚˜ì™”ë‹¤ê³  í•˜ì
$overhead$ëŠ” 25ì´ˆì´ê³  $SpeedUp$ì€ 4ë°°ê°€ ë‚˜ì˜¤ê³  $Efficiency$ëŠ” 0.8ì´ ë‚˜ì˜¨ë‹¤.
$Efficiency$ê°€ 1ì— ê°€ê¹ê²Œ ë‚˜ì˜¤ë©´ ë‚˜ì˜¬ìˆ˜ë¡ ì¢‹ì€ ìƒí™©ì¸ ê²ƒì´ë‹¤.

## Amdahl's Law

$f$ëŠ” ë³‘ë ¬í™” ê°€ëŠ¥í•œ ì‘ì—…ì˜ ë¹„ìœ¨ì„ ì–˜ê¸°í•˜ê³  $k$ëŠ” ì†ë„í–¥ìƒì§€ìˆ˜ë¥¼ ì˜ë¯¸í•œë‹¤.
$T\quad = (1-f)T\;+fT$
$T_k\quad = (1-f)T\;+\frac{f}{K}T$
ì´ ë‘ ì‹ì„ ê¸°ë°˜ìœ¼ë¡œ SpeedUpì„ ê³„ì‚°í•´ë³¸ë‹¤ë©´ ì•„ë˜ì™€ ê°™ì€ ì‹ì´ ë‚˜ì˜¤ê²Œ ëœë‹¤.
$$SpeedUp\quad=\frac{T}{T_k}\quad=\frac{1}{(1-f)\;+\frac{f}{K}}$$

1. Large $f$ Case
   ë³‘ë ¬ì²˜ë¦¬ ê°€ëŠ¥í•œ ë¶€ë¶„ì´ ë§ì€ ìƒí™©ì„ ì–˜ê¸°í•œë‹¤.
   ì• ì´ˆì— $f$ ì˜ ë¹„ìœ¨ì´ ì ë‹¤ë©´ ì„±ëŠ¥ í–¥ìƒì´ ë§ì´ ì§„í–‰ë˜ì§€ ì•Šì„ ê²ƒì´ë‹¤.
   -> ì½”ì–´ê°€ ë¬´í•œëŒ€ë¼ê³  í•´ë´¤ì ë³‘ë ¬ì²˜ë¦¬ ë¹„ìœ¨ì„ ë”°ë¼ê°€ê²Œ ë˜ëŠ” ê²ƒì´ë‹¤.
   Task-Parallel computationì€ ê³ ì „ ë³‘ë ¬ì²˜ë¦¬ ì½”ì–´ìˆ˜ê°€ ì ì–´ì•¼ì§€ ì¢‹ë‹¤.
2. Large $K$ Case
   ì½”ì–´ìˆ˜ë‚˜ í•œêº¼ë²ˆì— ì²˜ë¦¬í•  ìˆ˜ ìˆëŠ” ë°ì´í„° ë§ì€ ê²½ìš°ë¥¼ ìƒê°í•˜ë©´ ëœë‹¤.
   - Data-Parallel Computation(Kê°€ ì˜ í™•ì¥ ê°€ëŠ¥)
   - Task-Parallel Computation(ì‘ì—…ì˜ ìˆ˜ê°€ ê³ ì •ë˜ì–´ ìˆê¸° ë•Œë¬¸ì— Kê°€ ì˜ í™•ì¥ X)

### Scalability

- Strong Scaling - ë¬¸ì œì˜ í¬ê¸°ê°€ ê³ ì •ì¼ ë•Œ ì½”ì–´ìˆ˜ì— ë”°ë¼ì„œ ì–¼ë§ˆë‚˜ ë³€í•˜ëŠ”ì§€
- Weak Scaling - $Problem/p$ëŠ” ê³ ì • ì½”ì–´ìˆ˜ê°€ í™•ì¥ë¨ì— ë‹¤ë¼ ë¬¸ì œì˜ í¬ê¸°ë„ ì¦ê°€
  What would affect the scalability of an algorithm
  -> ì½”ì–´ê°€ ë§ëƒ ì ëƒì— ë”°ë¼ì„œ ë‹¤ë¥´ê²Œ ìƒê°í•´ì•¼ í•œë‹¤.

# Designing Parallel Algorithm

Identify hotspots
-> ê°€ì¥ ë§ì´ ì‹œê°„ì´ ë“œëŠ” ì—°ì‚° êµ¬ê°„ì„ ì°¾ëŠ”ë‹¤ by profile in sequential program
Identify bottleneck
-> ê¸°ëŒ€í–ˆëŠ”ë° ì˜ ì•ˆ ë˜ëŠ” ë¶€ë¶„ì´ ìˆë‹¤ë©´ ê±°ê¸°ê°€ bottleneck

# Computation

Try to decrease the ciritical path
-> reduce the sequential bottleneck
-> minimize loading balance
-> reduce data dependence

# Synchronization and Load Imbalance

1. Use Distributed Version instead of centralized version
2. Lock Free
3. Avoid coarse-grained task decomposition
4. higher priority to ciritical works

# Communication

non-deterministic
